use std::time::{Duration, Instant};
use std::collections::HashMap;
use embed_search::search::bm25::{BM25Engine, BM25Document, Token};
use embed_search::search::text_processor::CodeTextProcessor;

/// Implementation of the 6 MISSING BM25 stress tests with BRUTAL HONESTY
/// Each test interacts with real BM25Engine, measures actual performance,
/// and provides clear pass/fail criteria with detailed error messages.

#[cfg(test)]
mod missing_bm25_stress_tests {
    use super::*;

    /// Test 1: stress_incremental_update_impossibility
    /// REALITY CHECK: Test that updates require full reindex
    /// This test verifies BM25Engine handles incremental updates correctly
    #[test]
    fn stress_incremental_update_impossibility() {
        println!("=== STRESS TEST 1: INCREMENTAL UPDATE IMPOSSIBILITY ===");
        
        let mut engine = BM25Engine::new();
        let start_time = Instant::now();
        
        // Phase 1: Build initial index
        println!("Phase 1: Building initial index with 1000 documents");
        for i in 0..1000 {
            let doc = create_stress_document(&format!("initial_{}", i), vec!["function", "process", &format!("term_{}", i % 100)]);
            engine.add_document(doc).expect("Initial document add must succeed");
        }
        
        let initial_stats = engine.get_stats();
        let initial_time = start_time.elapsed();
        println!("Initial indexing: {} docs in {:?}", initial_stats.total_documents, initial_time);
        
        // Phase 2: Incremental updates - measure performance degradation
        println!("Phase 2: Adding 100 incremental documents");
        let incremental_start = Instant::now();
        
        for i in 1000..1100 {
            let doc = create_stress_document(&format!("incremental_{}", i), vec!["function", "update", &format!("new_term_{}", i)]);
            match engine.add_document(doc) {
                Ok(_) => {},
                Err(e) => {
                    println!("ðŸš¨ INCREMENTAL UPDATE FAILURE: Document {} failed to add: {}", i, e);
                    println!("   BM25Engine hit limits after {} existing documents", initial_stats.total_documents);
                    println!("ðŸŽ¯ This indicates the engine may have capacity or performance limits");
                    break; // Stop adding more documents if we hit limits
                }
            }
        }
        
        let incremental_time = incremental_start.elapsed();
        let final_stats = engine.get_stats();
        
        // Verify correctness
        if final_stats.total_documents != 1100 {
            panic!("DOCUMENT COUNT CORRUPTION: Expected 1100 documents, got {}. \
                   Incremental updates corrupted document tracking.", final_stats.total_documents);
        }
        
        // Performance analysis - incremental should be slower if no optimization
        let avg_initial_time = initial_time.as_millis() as f64 / 1000.0;
        let avg_incremental_time = incremental_time.as_millis() as f64 / 100.0;
        
        println!("Performance: Initial avg {:.2}ms/doc, Incremental avg {:.2}ms/doc", 
                avg_initial_time, avg_incremental_time);
        
        // Test search accuracy after incremental updates
        match engine.search("function", 1100) {
            Ok(results) => {
                // All 1100 documents contain the term "function"
                if results.len() != 1100 {
                    panic!("SEARCH ACCURACY FAILURE: Found {} results for 'function' but should find all 1100 documents. \
                           Incremental updates broke search index integrity.", results.len());
                }
                
                // Verify scores are mathematically valid
                for (idx, result) in results.iter().enumerate() {
                    if !result.score.is_finite() || result.score < 0.0 {
                        panic!("MATHEMATICAL CORRUPTION: Result {} has invalid score {}. \
                               Incremental updates corrupted BM25 calculations.", idx, result.score);
                    }
                }
                
                println!("âœ“ Incremental updates succeeded: {} documents indexed, search returning {} results",
                        final_stats.total_documents, results.len());
            },
            Err(e) => {
                panic!("SEARCH FAILURE AFTER INCREMENTAL UPDATE: Search failed after adding documents: {}. \
                       Incremental updates completely broke search functionality.", e);
            }
        }
        
        // Memory analysis - detect if memory usage is reasonable
        if final_stats.total_terms == 0 {
            panic!("VOCABULARY CORRUPTION: No terms recorded despite adding 1100 documents. \
                   Incremental updates destroyed vocabulary tracking.");
        }
        
        println!("Final vocabulary: {} terms for {} documents", final_stats.total_terms, final_stats.total_documents);
        println!("âœ“ INCREMENTAL UPDATE TEST PASSED - Engine handles incremental updates correctly");
    }

    /// Test 2: stress_tokenization_catastrophe  
    /// REALITY CHECK: Complex text breaking tokenization
    /// Tests BM25Engine with malformed and complex tokenization scenarios
    #[test]
    fn stress_tokenization_catastrophe() {
        println!("=== STRESS TEST 2: TOKENIZATION CATASTROPHE ===");
        
        let mut engine = BM25Engine::new();
        
        // Create extreme length strings
        let extreme_long = "x".repeat(10000);
        let extreme_prefix = format!("{}_test", "a".repeat(5000));
        
        // Catastrophic tokenization scenarios
        let catastrophic_cases = vec![
            // Empty and whitespace edge cases
            ("", "Empty string"),
            ("   \t\n\r  ", "Pure whitespace"),
            
            // Unicode catastrophes
            ("ðŸ”¥ðŸ’»âš¡ðŸš€ðŸŽ¯", "Pure emoji sequence"),
            ("cafÃ©\u{200B}naÃ¯ve\u{FEFF}rÃ©sumÃ©", "Unicode with zero-width characters"),
            ("Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ_Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°_Ð´Ð°Ð½Ð½Ñ‹Ñ…", "Cyrillic underscore mixing"),
            ("æ•°æ®å¤„ç†å‡½æ•°\n\tæ•°æ®åº“æŸ¥è¯¢", "Chinese with mixed whitespace"),
            
            // Malformed token boundaries
            ("word1word2word3word4word5", "No separators between words"),
            ("a.b.c.d.e.f.g.h.i.j.k.l.m.n.o.p", "Excessive punctuation"),
            ("word\x00null\x01char\x02test", "Control characters"),
            
            // Extreme length tokens
            (&extreme_long, "10k character single token"),
            (&extreme_prefix, "5k character prefix"),
            
            // Mixed script catastrophes
            ("englishæ—¥æœ¬èªžØ§Ù„Ø¹Ø±Ø¨ÙŠØ©Ñ€ÑƒÑÑÐºÐ¸Ð¹í•œêµ­ì–´", "5 scripts without separators"),
            ("HTTP://EXAMPLE.COM/PATH?QUERY=VALUE&PARAM=DATA", "URL-like all caps"),
        ];
        
        for (idx, (text, description)) in catastrophic_cases.iter().enumerate() {
            println!("Testing catastrophic case {}: {}", idx + 1, description);
            
            // Create tokens manually to bypass text processor limitations
            let tokens = if text.is_empty() {
                vec![] // Empty token list
            } else {
                // Split by character for maximum catastrophe
                text.chars().enumerate().map(|(pos, ch)| Token {
                    text: ch.to_string(),
                    position: pos,
                    importance_weight: 1.0,
                }).collect()
            };
            
            let doc = BM25Document {
                id: format!("catastrophic_{}", idx),
                file_path: format!("catastrophic_{}.txt", idx),
                chunk_index: 0,
                tokens,
                start_line: 0,
                end_line: 1,
                language: Some("mixed".to_string()),
            };
            
            // Test document addition
            match engine.add_document(doc) {
                Ok(_) => {
                    println!("âœ“ Catastrophic case {} added successfully", idx + 1);
                    
                    // Verify engine state after catastrophic input
                    let stats = engine.get_stats();
                    if stats.total_documents <= idx {
                        panic!("DOCUMENT TRACKING FAILURE: Stats show {} documents but {} were added. \
                               Catastrophic tokenization corrupted document counting.", 
                               stats.total_documents, idx + 1);
                    }
                },
                Err(e) => {
                    // Some catastrophic inputs might legitimately fail
                    if text.is_empty() {
                        println!("Empty document rejected (acceptable): {}", e);
                    } else {
                        panic!("TOKENIZATION CATASTROPHE FAILURE: Cannot add document with {}: {}. \
                               BM25 engine cannot handle real-world text complexity.", description, e);
                    }
                }
            }
        }
        
        // Test searching with catastrophic queries
        let catastrophic_queries = vec![
            "ðŸ”¥", "ðŸ’»", "cafÃ©", "æ•°æ®", "Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ",
            "x", "a", // Single characters from extreme cases
        ];
        
        for query in catastrophic_queries {
            match engine.search(query, 10) {
                Ok(results) => {
                    println!("âœ“ Catastrophic search '{}': {} results", query, results.len());
                    
                    // Verify result validity
                    for result in results {
                        if !result.score.is_finite() || result.score < 0.0 {
                            panic!("CATASTROPHIC SEARCH CORRUPTION: Query '{}' produced invalid score {}. \
                                   Complex tokenization broke mathematical integrity.", query, result.score);
                        }
                    }
                },
                Err(e) => {
                    // Some catastrophic queries might fail - check if it's reasonable
                    println!("Catastrophic search '{}' failed (may be acceptable): {}", query, e);
                }
            }
        }
        
        println!("âœ“ TOKENIZATION CATASTROPHE TEST PASSED - Engine survives complex tokenization");
    }

    /// Test 3: stress_persistence_failure_validation
    /// REALITY CHECK: Data loss on restart  
    /// Validates that BM25Engine persistence limitations are clearly documented
    #[test]
    fn stress_persistence_failure_validation() {
        println!("=== STRESS TEST 3: PERSISTENCE FAILURE VALIDATION ===");
        
        // Phase 1: Build substantial index
        let mut engine1 = BM25Engine::new();
        println!("Building substantial index for persistence test");
        
        let important_data = vec![
            ("critical_function", vec!["critical", "function", "important", "data"]),
            ("user_authentication", vec!["user", "auth", "login", "security"]),
            ("database_query", vec!["database", "query", "sql", "performance"]),
            ("api_endpoint", vec!["api", "endpoint", "rest", "service"]),
        ];
        
        for (id, terms) in &important_data {
            let doc = create_stress_document(id, terms.clone());
            engine1.add_document(doc).expect("Critical document add must succeed");
        }
        
        let pre_restart_stats = engine1.get_stats();
        println!("Pre-restart: {} documents, {} terms", 
                pre_restart_stats.total_documents, pre_restart_stats.total_terms);
        
        // Verify search works before "restart"
        let pre_search = engine1.search("critical", 10).expect("Pre-restart search must work");
        if pre_search.is_empty() {
            panic!("PRE-RESTART SEARCH FAILURE: Cannot find 'critical' before restart. \
                   Test setup is invalid.");
        }
        
        println!("Pre-restart search for 'critical': {} results", pre_search.len());
        
        // Phase 2: Simulate restart by creating new engine instance
        println!("Simulating application restart...");
        let engine2 = BM25Engine::new();
        
        let post_restart_stats = engine2.get_stats();
        println!("Post-restart: {} documents, {} terms", 
                post_restart_stats.total_documents, post_restart_stats.total_terms);
        
        // Phase 3: Validate data loss
        if post_restart_stats.total_documents != 0 {
            panic!("IMPOSSIBLE PERSISTENCE: Found {} documents after restart with no persistence mechanism. \
                   This indicates a fundamental bug in engine isolation or test design.", 
                   post_restart_stats.total_documents);
        }
        
        if post_restart_stats.total_terms != 0 {
            panic!("VOCABULARY PERSISTENCE BUG: Found {} terms after restart with no persistence. \
                   Engine instances are not properly isolated.", post_restart_stats.total_terms);
        }
        
        // Phase 4: Validate search failure with empty index
        match engine2.search("critical", 10) {
            Ok(results) => {
                if !results.is_empty() {
                    panic!("IMPOSSIBLE SEARCH RESULTS: Found {} results in empty engine after restart. \
                           Engine state is contaminated between instances.", results.len());
                }
                println!("âœ“ Post-restart search correctly returns no results");
            },
            Err(e) => {
                println!("âœ“ Post-restart search correctly fails: {}", e);
            }
        }
        
        // Phase 5: Validate recovery capability
        println!("Testing recovery by re-adding data...");
        let mut engine3 = BM25Engine::new();
        
        // Re-add critical data
        for (id, terms) in &important_data {
            let doc = create_stress_document(id, terms.clone());
            engine3.add_document(doc).expect("Recovery document add must succeed");
        }
        
        let recovery_search = engine3.search("critical", 10).expect("Recovery search must work");
        if recovery_search.is_empty() {
            panic!("RECOVERY FAILURE: Cannot recover functionality after re-adding data. \
                   Engine has persistent corruption.");
        }
        
        println!("âœ“ Recovery successful: {} results for 'critical'", recovery_search.len());
        
        // CONCLUSION
        println!("PERSISTENCE LIMITATION CONFIRMED:");
        println!("- BM25Engine is in-memory only");
        println!("- All data lost on restart/drop");
        println!("- Recovery requires full re-indexing");
        println!("- This is an architectural limitation, not a bug");
        println!("âœ“ PERSISTENCE FAILURE VALIDATION PASSED - Behavior is as expected");
    }

    /// Test 4: stress_length_bias_exposure
    /// REALITY CHECK: Document length bias issues
    /// Tests BM25 length normalization with extreme length differences
    #[test]
    fn stress_length_bias_exposure() {
        println!("=== STRESS TEST 4: LENGTH BIAS EXPOSURE ===");
        
        let mut engine = BM25Engine::new();
        
        // Create documents with extreme length differences
        let target_term = "target_bias_test";
        
        // Ultra-short document (2 terms)
        let ultra_short = BM25Document {
            id: "ultra_short".to_string(),
            file_path: "ultra_short.rs".to_string(),
            chunk_index: 0,
            tokens: vec![
                Token { text: target_term.to_string(), position: 0, importance_weight: 1.0 },
                Token { text: "end".to_string(), position: 1, importance_weight: 1.0 },
            ],
            start_line: 0,
            end_line: 1,
            language: Some("rust".to_string()),
        };
        
        // Medium document (100 terms)
        let mut medium_tokens = vec![
            Token { text: target_term.to_string(), position: 0, importance_weight: 1.0 },
        ];
        for i in 1..100 {
            medium_tokens.push(Token { 
                text: format!("medium_filler_{}", i), 
                position: i, 
                importance_weight: 1.0 
            });
        }
        let medium_doc = BM25Document {
            id: "medium".to_string(),
            file_path: "medium.rs".to_string(),
            chunk_index: 0,
            tokens: medium_tokens,
            start_line: 0,
            end_line: 50,
            language: Some("rust".to_string()),
        };
        
        // Ultra-long document (10,000 terms)
        let mut ultra_long_tokens = vec![
            Token { text: target_term.to_string(), position: 0, importance_weight: 1.0 },
        ];
        for i in 1..10000 {
            ultra_long_tokens.push(Token { 
                text: format!("ultra_long_filler_{}", i), 
                position: i, 
                importance_weight: 1.0 
            });
        }
        let ultra_long = BM25Document {
            id: "ultra_long".to_string(),
            file_path: "ultra_long.rs".to_string(), 
            chunk_index: 0,
            tokens: ultra_long_tokens,
            start_line: 0,
            end_line: 5000,
            language: Some("rust".to_string()),
        };
        
        // Add all documents
        engine.add_document(ultra_short).expect("Ultra short doc add must succeed");
        engine.add_document(medium_doc).expect("Medium doc add must succeed");
        engine.add_document(ultra_long).expect("Ultra long doc add must succeed");
        
        let stats = engine.get_stats();
        println!("Added {} documents, average length: {:.1}", stats.total_documents, stats.avg_document_length);
        
        // Search and analyze length bias
        let results = engine.search(target_term, 10).expect("Length bias search must succeed");
        
        if results.len() != 3 {
            panic!("LENGTH BIAS SEARCH FAILURE: Expected 3 results, got {}. \
                   All documents contain the target term.", results.len());
        }
        
        let mut scores = HashMap::new();
        for result in &results {
            scores.insert(result.doc_id.clone(), result.score);
            println!("Document '{}': score = {:.4}", result.doc_id, result.score);
            
            if !result.score.is_finite() || result.score <= 0.0 {
                panic!("LENGTH BIAS MATHEMATICAL ERROR: Document '{}' has invalid score {}. \
                       Length normalization corrupted calculations.", result.doc_id, result.score);
            }
        }
        
        let short_score = scores["ultra_short"];
        let medium_score = scores["medium"];
        let long_score = scores["ultra_long"];
        
        // Analyze length normalization effectiveness
        println!("Score analysis:");
        println!("- Ultra short (2 terms): {:.4}", short_score);
        println!("- Medium (100 terms): {:.4}", medium_score);
        println!("- Ultra long (10k terms): {:.4}", long_score);
        
        // BM25 should penalize very long documents due to length normalization
        if long_score >= medium_score && medium_score >= short_score {
            panic!("LENGTH BIAS FAILURE: Longer documents scored higher (short:{:.4} < medium:{:.4} < long:{:.4}). \
                   BM25 length normalization is not working. b parameter may be 0 or broken.", 
                   short_score, medium_score, long_score);
        }
        
        // Verify reasonable length normalization
        let short_to_long_ratio = short_score / long_score;
        println!("Short-to-long score ratio: {:.2}", short_to_long_ratio);
        
        if short_to_long_ratio < 1.1 {
            panic!("INSUFFICIENT LENGTH NORMALIZATION: Short document only scored {:.2}x higher than ultra-long. \
                   BM25 b parameter ({}) may be too low for effective length normalization.", 
                   short_to_long_ratio, stats.b);
        }
        
        if short_to_long_ratio > 100.0 {
            panic!("EXCESSIVE LENGTH NORMALIZATION: Short document scored {:.2}x higher than ultra-long. \
                   Length normalization may be too aggressive.", short_to_long_ratio);
        }
        
        // Test with multiple term frequencies
        let multi_term_query = format!("{} end filler", target_term);
        let multi_results = engine.search(&multi_term_query, 10).expect("Multi-term search must succeed");
        
        if multi_results.is_empty() {
            panic!("MULTI-TERM LENGTH BIAS FAILURE: No results for multi-term query. \
                   Length bias broke complex query processing.");
        }
        
        println!("Multi-term query results: {}", multi_results.len());
        for result in &multi_results {
            println!("- {}: {:.4}", result.doc_id, result.score);
        }
        
        println!("âœ“ LENGTH BIAS EXPOSURE TEST PASSED - BM25 length normalization working correctly");
    }

    /// Test 5: stress_unicode_tokenization_destruction
    /// REALITY CHECK: International text failures
    /// Tests BM25Engine with complex Unicode that breaks typical tokenization
    #[test]
    fn stress_unicode_tokenization_destruction() {
        println!("=== STRESS TEST 5: UNICODE TOKENIZATION DESTRUCTION ===");
        
        let mut engine = BM25Engine::new();
        
        // Destructive Unicode test cases - designed to break tokenizers
        let destructive_cases = vec![
            // Normalization catastrophes - same visual text, different encodings
            ("cafÃ©", "NFC normalized cafÃ©"),
            ("cafe\u{301}", "NFD decomposed cafÃ©"), 
            ("caf\u{00E9}", "Single codepoint Ã©"),
            
            // Bidirectional text catastrophes
            ("Hello\u{202E}world\u{202C}test", "RLO/PDF embedding attack"),
            ("user\u{061C}pass\u{061C}word", "Arabic letter mark injection"),
            
            // Zero-width catastrophes  
            ("func\u{200B}tion", "Zero-width space injection"),
            ("test\u{FEFF}data", "BOM injection"),
            ("var\u{200C}name", "Zero-width non-joiner"),
            
            // Script mixing catastrophes
            ("Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ_process_ãƒ‡ãƒ¼ã‚¿_Ù…Ø¹Ø§Ù„Ø¬", "4 scripts in identifier"), 
            ("user\u{0300}\u{0301}\u{0302}name", "Heavy diacritic stacking"),
            
            // Emoji/symbol catastrophes
            ("ðŸš€functionðŸ’»testâš¡code", "Emoji without separators"),
            ("ðŸ‘¨â€ðŸ’»ðŸ‘©â€ðŸ”¬ðŸ§‘â€ðŸŽ¨", "Multi-part emoji sequence"),
            ("âˆ‘âˆ«âˆ‚âˆ‡Î±â‰ˆâ‰ â‰¤â‰¥", "Mathematical symbol sequence"),
            
            // Control character injection
            ("func\rtion\ntest\tdata", "CR/LF/Tab injection"),
            ("test\x08\x7Fdata", "Backspace/DEL injection"),
            
            // Extreme Unicode ranges
            ("ð•—ð•¦ð•Ÿð•”ð•¥ð•šð• ð•Ÿ", "Mathematical script (U+1D500+)"),
            ("ðŸ„µðŸ…„ðŸ„½ðŸ„²ðŸ…ƒðŸ„¸ðŸ„¾ðŸ„½", "Enclosed alphanumerics"),
        ];
        
        for (idx, (text, description)) in destructive_cases.iter().enumerate() {
            println!("Testing destructive Unicode {}: {}", idx + 1, description);
            
            // Use text processor for realistic tokenization
            let processor = CodeTextProcessor::new();
            let processed_tokens = processor.process_text(text, "mixed");
            
            if processed_tokens.is_empty() {
                println!("WARNING: Text processor produced no tokens for: {}", description);
                // Create at least one token for BM25 testing
                let bm25_tokens = vec![Token {
                    text: text.to_string(),
                    position: 0,
                    importance_weight: 1.0,
                }];
                
                let doc = BM25Document {
                    id: format!("unicode_destructive_{}", idx),
                    file_path: format!("unicode_{}.txt", idx),
                    chunk_index: 0,
                    tokens: bm25_tokens,
                    start_line: 0,
                    end_line: 1,
                    language: Some("mixed".to_string()),
                };
                
                match engine.add_document(doc) {
                    Ok(_) => println!("âœ“ Raw Unicode document added"),
                    Err(e) => {
                        panic!("UNICODE DESTRUCTION FAILURE: Cannot add raw Unicode '{}' ({}): {}. \
                               BM25 engine cannot handle Unicode edge cases.", text, description, e);
                    }
                }
            } else {
                // Convert processed tokens to BM25 format
                let bm25_tokens: Vec<Token> = processed_tokens.iter().enumerate()
                    .map(|(pos, pt)| Token {
                        text: pt.text.clone(),
                        position: pos,
                        importance_weight: pt.importance_weight,
                    }).collect();
                
                let doc = BM25Document {
                    id: format!("unicode_processed_{}", idx),
                    file_path: format!("unicode_processed_{}.txt", idx),
                    chunk_index: 0,
                    tokens: bm25_tokens,
                    start_line: 0,
                    end_line: 1,
                    language: Some("mixed".to_string()),
                };
                
                match engine.add_document(doc) {
                    Ok(_) => {
                        println!("âœ“ Processed Unicode document added ({} tokens)", processed_tokens.len());
                    },
                    Err(e) => {
                        panic!("PROCESSED UNICODE FAILURE: Cannot add processed Unicode '{}' ({}): {}. \
                               BM25 processing failed after tokenization.", text, description, e);
                    }
                }
            }
        }
        
        let stats = engine.get_stats();
        println!("Added {} Unicode documents, vocabulary: {} terms", 
                stats.total_documents, stats.total_terms);
        
        // Test searching with destructive Unicode queries
        let destructive_queries = vec![
            "cafÃ©", "function", "test", "data", "user", "ðŸš€", "ðŸ’»", 
            "func", "tion", // Partial matches from broken tokens
            "Ð¿Ñ€Ð¾Ñ†ÐµÑÑ", "ãƒ‡ãƒ¼ã‚¿", "Ù…Ø¹Ø§Ù„Ø¬", // Non-Latin scripts
        ];
        
        for query in destructive_queries {
            match engine.search(query, 5) {
                Ok(results) => {
                    println!("âœ“ Unicode search '{}': {} results", query, results.len());
                    
                    // Validate search result integrity
                    for (idx, result) in results.iter().enumerate() {
                        if !result.score.is_finite() || result.score < 0.0 {
                            panic!("UNICODE SEARCH CORRUPTION: Query '{}' result {} has invalid score {}. \
                                   Unicode processing corrupted mathematical calculations.", 
                                   query, idx, result.score);
                        }
                        
                        // Check document ID validity
                        if result.doc_id.is_empty() {
                            panic!("UNICODE RESULT CORRUPTION: Query '{}' produced empty document ID. \
                                   Unicode processing corrupted result tracking.", query);
                        }
                    }
                },
                Err(e) => {
                    println!("Unicode search '{}' failed: {}", query, e);
                    // Some failures may be acceptable for extreme Unicode
                }
            }
        }
        
        // Test for Unicode normalization consistency
        let normalization_tests = vec![
            ("cafÃ©", "Composed form"),
            ("cafe\u{301}", "Decomposed form"),
        ];
        
        for (query, form) in normalization_tests {
            match engine.search(query, 10) {
                Ok(results) => {
                    println!("Normalization test '{}' ({}): {} results", query, form, results.len());
                },
                Err(e) => {
                    println!("Normalization test '{}' failed: {}", query, e);
                }
            }
        }
        
        println!("âœ“ UNICODE TOKENIZATION DESTRUCTION TEST PASSED - Engine survives Unicode complexity");
    }

    /// Test 6: stress_vocabulary_overflow_limits
    /// REALITY CHECK: Memory exhaustion with 100k+ terms
    /// Tests BM25Engine memory handling with massive vocabulary
    #[test]
    fn stress_vocabulary_overflow_limits() {
        println!("=== STRESS TEST 6: VOCABULARY OVERFLOW LIMITS ===");
        
        let mut engine = BM25Engine::new();
        let start_memory = get_memory_usage(); 
        let start_time = Instant::now();
        
        // Target: 100k unique terms to test memory limits
        const TARGET_VOCAB_SIZE: usize = 100_000;
        const BATCH_SIZE: usize = 5_000;  // Smaller batches for memory monitoring
        const TERMS_PER_DOC: usize = 100;
        
        println!("Starting vocabulary overflow test - targeting {} unique terms", TARGET_VOCAB_SIZE);
        println!("Initial memory usage: {:.1} MB", start_memory);
        
        let mut total_terms_added = 0;
        let mut doc_counter = 0;
        let mut last_memory_check = start_memory;
        
        while total_terms_added < TARGET_VOCAB_SIZE {
            let batch_start = Instant::now();
            let terms_in_batch = std::cmp::min(TERMS_PER_DOC, TARGET_VOCAB_SIZE - total_terms_added);
            
            // Create document with unique terms
            let mut tokens = Vec::with_capacity(terms_in_batch);
            for i in 0..terms_in_batch {
                let unique_term = format!("vocab_overflow_term_{:06}", total_terms_added + i);
                tokens.push(Token {
                    text: unique_term,
                    position: i,
                    importance_weight: 1.0,
                });
            }
            
            let doc = BM25Document {
                id: format!("vocab_doc_{:05}", doc_counter),
                file_path: format!("vocab_{}.rs", doc_counter),
                chunk_index: 0,
                tokens,
                start_line: 0,
                end_line: 100,
                language: Some("rust".to_string()),
            };
            
            // Attempt to add document
            match engine.add_document(doc) {
                Ok(_) => {
                    total_terms_added += terms_in_batch;
                    doc_counter += 1;
                    
                    // Memory monitoring every 10k terms
                    if total_terms_added % 10_000 == 0 {
                        let current_memory = get_memory_usage();
                        let memory_increase = current_memory - last_memory_check;
                        let stats = engine.get_stats();
                        
                        println!("Progress: {} terms, {} docs, {:.1} MB (+{:.1} MB), vocab: {}", 
                                total_terms_added, stats.total_documents, 
                                current_memory, memory_increase, stats.total_terms);
                        
                        // Check for memory explosion
                        if memory_increase > 100.0 { // More than 100MB increase per 10k terms
                            println!("WARNING: High memory usage increase: {:.1} MB for 10k terms", memory_increase);
                        }
                        
                        // Check vocabulary tracking accuracy
                        if stats.total_terms == 0 {
                            panic!("VOCABULARY TRACKING FAILURE: Engine reports 0 terms but {} were added. \
                                   Memory management is corrupted.", total_terms_added);
                        }
                        
                        // Catastrophic memory usage check
                        if current_memory > 2000.0 { // More than 2GB
                            panic!("MEMORY OVERFLOW CATASTROPHE: Memory usage {:.1} MB exceeds reasonable limits. \
                                   BM25 engine has memory leak or inefficient storage.", current_memory);
                        }
                        
                        last_memory_check = current_memory;
                    }
                    
                    let batch_time = batch_start.elapsed();
                    if batch_time > Duration::from_secs(5) {
                        println!("WARNING: Batch {} took {:?} - performance degrading", doc_counter, batch_time);
                    }
                },
                Err(e) => {
                    let current_memory = get_memory_usage();
                    println!("VOCABULARY OVERFLOW LIMIT REACHED at {} terms: {}", total_terms_added, e);
                    println!("Final memory usage: {:.1} MB", current_memory);
                    
                    if total_terms_added < 50_000 {
                        panic!("PREMATURE VOCABULARY FAILURE: Failed at only {} terms: {}. \
                               This is far below reasonable vocabulary limits. Expected at least 50k terms.", 
                               total_terms_added, e);
                    }
                    
                    println!("Vocabulary limit reached at {} terms (acceptable limit)", total_terms_added);
                    break;
                }
            }
            
            // Emergency exit for infinite loops
            if start_time.elapsed() > Duration::from_secs(300) { // 5 minute timeout
                panic!("VOCABULARY OVERFLOW TIMEOUT: Test ran for 5+ minutes without completing. \
                       Performance is unacceptable or system is hanging.");
            }
        }
        
        // Performance validation with large vocabulary
        let final_stats = engine.get_stats();
        let final_memory = get_memory_usage();
        let total_time = start_time.elapsed();
        
        println!("=== FINAL VOCABULARY OVERFLOW RESULTS ===");
        println!("Documents: {}", final_stats.total_documents);
        println!("Vocabulary size: {}", final_stats.total_terms);
        println!("Memory usage: {:.1} MB (increase: {:.1} MB)", 
                final_memory, final_memory - start_memory);
        println!("Total time: {:?}", total_time);
        
        if final_stats.total_terms > 0 {
            // Search performance test with large vocabulary
            println!("Testing search performance with large vocabulary...");
            
            let last_term = format!("vocab_overflow_term_{:06}", final_stats.total_terms.saturating_sub(1));
            let search_terms = vec![
                "vocab_overflow_term_000001", // First term
                "vocab_overflow_term_050000", // Middle term  
                &last_term, // Last term
                "nonexistent_term", // Miss case
            ];
            
            for term in search_terms {
                let search_start = Instant::now();
                match engine.search(term, 10) {
                    Ok(results) => {
                        let search_time = search_start.elapsed();
                        println!("Search '{}': {} results in {:?}", term, results.len(), search_time);
                        
                        if search_time > Duration::from_secs(1) {
                            panic!("VOCABULARY SEARCH PERFORMANCE FAILURE: Search for '{}' took {:?} with {} terms. \
                                   Performance is unacceptable for large vocabularies.", 
                                   term, search_time, final_stats.total_terms);
                        }
                        
                        // Validate result quality
                        for (idx, result) in results.iter().enumerate() {
                            if !result.score.is_finite() || result.score < 0.0 {
                                panic!("VOCABULARY SEARCH CORRUPTION: Result {} for '{}' has invalid score {}. \
                                       Large vocabulary corrupted calculations.", idx, term, result.score);
                            }
                        }
                    },
                    Err(e) => {
                        if term == "nonexistent_term" {
                            println!("âœ“ Non-existent term correctly failed: {}", e);
                        } else {
                            panic!("VOCABULARY SEARCH FAILURE: Cannot search for existing term '{}': {}. \
                                   Large vocabulary broke search functionality.", term, e);
                        }
                    }
                }
            }
        }
        
        // Memory efficiency analysis
        if final_stats.total_terms > 0 && final_memory > start_memory {
            let memory_per_term = (final_memory - start_memory) * 1024.0 * 1024.0 / final_stats.total_terms as f64;
            println!("Memory efficiency: {:.1} bytes per term", memory_per_term);
            
            if memory_per_term > 1000.0 { // More than 1KB per term
                panic!("MEMORY EFFICIENCY FAILURE: Using {:.1} bytes per term. \
                       BM25 storage is extremely inefficient.", memory_per_term);
            }
        }
        
        println!("âœ“ VOCABULARY OVERFLOW LIMITS TEST PASSED - Engine handles large vocabularies appropriately");
    }

    // Helper functions

    fn create_stress_document(id: &str, terms: Vec<&str>) -> BM25Document {
        BM25Document {
            id: id.to_string(),
            file_path: format!("{}.rs", id),
            chunk_index: 0,
            tokens: terms.iter().enumerate().map(|(pos, &term)| Token {
                text: term.to_string(),
                position: pos,
                importance_weight: 1.0,
            }).collect(),
            start_line: 0,
            end_line: 10,
            language: Some("rust".to_string()),
        }
    }

    #[cfg(target_os = "windows")]
    fn get_memory_usage() -> f64 {
        use std::process::Command;
        
        let output = Command::new("tasklist")
            .arg("/FI")
            .arg(&format!("PID eq {}", std::process::id()))
            .arg("/FO")
            .arg("CSV")
            .output();
        
        if let Ok(output) = output {
            let content = String::from_utf8_lossy(&output.stdout);
            // Parse CSV output for memory usage
            // This is a simplified approximation
            if let Some(line) = content.lines().nth(1) {
                if let Some(mem_field) = line.split(',').nth(4) {
                    let mem_str = mem_field.trim_matches('"').replace(",", "");
                    if let Some(mem_kb_str) = mem_str.strip_suffix(" K") {
                        if let Ok(mem_kb) = mem_kb_str.parse::<f64>() {
                            return mem_kb / 1024.0; // Convert KB to MB
                        }
                    }
                }
            }
        }
        
        // Fallback approximation
        0.0
    }
    
    #[cfg(not(target_os = "windows"))]
    fn get_memory_usage() -> f64 {
        use std::fs;
        
        // Read /proc/self/status for memory info on Unix-like systems
        if let Ok(status) = fs::read_to_string("/proc/self/status") {
            for line in status.lines() {
                if line.starts_with("VmRSS:") {
                    if let Some(kb_str) = line.split_whitespace().nth(1) {
                        if let Ok(kb) = kb_str.parse::<f64>() {
                            return kb / 1024.0; // Convert KB to MB
                        }
                    }
                }
            }
        }
        
        // Fallback approximation
        0.0
    }
}