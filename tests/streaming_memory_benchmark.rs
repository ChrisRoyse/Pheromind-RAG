/// STREAMING MEMORY BENCHMARK - VERIFICATION TESTS
/// 
/// This test suite validates that the streaming tensor loader prevents V8 heap crashes
/// and achieves the target memory reduction of >80%.

use std::sync::Arc;
use std::time::{Duration, Instant};
use embed::embedding::streaming_core::StreamingGGUFLoader;
use embed::embedding::streaming_nomic_integration::StreamingNomicEmbedder;
use embed::utils::memory_monitor::{MemoryMonitor, get_system_memory_info};

#[tokio::test]
async fn test_streaming_memory_constraints() {
    println!("üß™ Testing streaming memory constraints");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    let initial_usage = monitor.current_usage_mb();
    
    // Create streaming loader (should use minimal memory)
    let result = StreamingGGUFLoader::new("test_model.gguf", monitor.clone());
    
    match result {
        Ok(_loader) => {
            let final_usage = monitor.current_usage_mb();
            let memory_used = final_usage - initial_usage;
            
            println!("  ‚úÖ Streaming loader created");
            println!("  üìä Memory used: {} MB", memory_used);
            
            // CRITICAL: Should use <2MB for loader itself
            assert!(memory_used < 2, "Streaming loader used {} MB, expected <2MB", memory_used);
        }
        Err(e) => {
            println!("  ‚ÑπÔ∏è  Loader creation failed (expected if no test model): {}", e);
        }
    }
}

#[tokio::test]
async fn test_memory_allocation_limits() {
    println!("üß™ Testing memory allocation limits");
    
    let monitor = Arc::new(MemoryMonitor::new(100, 80)); // 100MB limit
    
    // Test chunk size limits
    assert_eq!(StreamingGGUFLoader::CHUNK_SIZE, 65536);
    assert_eq!(StreamingGGUFLoader::DECODE_SIZE, 16384);
    assert_eq!(StreamingGGUFLoader::MAX_WORKING_MEMORY, 1048576);
    
    println!("  ‚úÖ Chunk size: {} bytes (64KB)", StreamingGGUFLoader::CHUNK_SIZE);
    println!("  ‚úÖ Decode buffer: {} floats (64KB)", StreamingGGUFLoader::DECODE_SIZE);
    println!("  ‚úÖ Max working memory: {} bytes (1MB)", StreamingGGUFLoader::MAX_WORKING_MEMORY);
    
    // Verify memory limits are enforced
    assert!(StreamingGGUFLoader::CHUNK_SIZE <= 65536, "Chunk size exceeds 64KB limit");
    assert!(StreamingGGUFLoader::MAX_WORKING_MEMORY <= 1048576, "Working memory exceeds 1MB limit");
}

#[tokio::test] 
async fn test_system_memory_monitoring() {
    println!("üß™ Testing system memory monitoring");
    
    if let Some(sys_info) = get_system_memory_info() {
        println!("  üìä System memory: {} MB total, {} MB available", 
                sys_info.total_mb, sys_info.available_mb);
        println!("  üìä Memory usage: {:.1}%", sys_info.used_percent);
        
        // Warn if system memory is low
        if sys_info.is_low_memory() {
            println!("  ‚ö†Ô∏è  Low system memory detected");
        }
        
        if sys_info.is_critical_memory() {
            println!("  üö® Critical system memory - tests may fail");
        }
    } else {
        println!("  ‚ÑπÔ∏è  System memory info not available on this platform");
    }
}

#[tokio::test]
async fn test_streaming_vs_traditional_memory_usage() {
    println!("üß™ Comparing streaming vs traditional memory usage");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    
    // Simulate traditional approach (large allocation)
    let traditional_size = 100 * 1024 * 1024; // 100MB
    println!("  üìä Traditional approach would allocate: {} MB", traditional_size / 1_048_576);
    
    // Streaming approach
    let streaming_size = StreamingGGUFLoader::MAX_WORKING_MEMORY;
    println!("  üìä Streaming approach allocates: {} MB", streaming_size / 1_048_576);
    
    let reduction_percent = ((traditional_size - streaming_size) as f64 / traditional_size as f64) * 100.0;
    println!("  üéØ Memory reduction: {:.1}%", reduction_percent);
    
    // CRITICAL: Should achieve >80% reduction
    assert!(reduction_percent > 80.0, "Memory reduction {:.1}% below 80% target", reduction_percent);
    
    // CRITICAL: Streaming should use <2MB
    assert!(streaming_size < 2 * 1_048_576, "Streaming memory {} exceeds 2MB", streaming_size);
}

#[tokio::test]
async fn test_v8_crash_prevention() {
    println!("üß™ Testing V8 crash prevention");
    
    // This test verifies that large allocations are blocked
    let monitor = Arc::new(MemoryMonitor::new(50, 80)); // 50MB limit
    
    // Try to allocate too much memory
    let large_allocation = 60 * 1024 * 1024; // 60MB
    let can_allocate = monitor.can_allocate(large_allocation);
    
    println!("  üìä Attempting {} MB allocation", large_allocation / 1_048_576);
    println!("  üõ°Ô∏è  Allocation blocked: {}", !can_allocate);
    
    // CRITICAL: Should block large allocations
    assert!(!can_allocate, "Large allocation should be blocked");
    
    // Small allocations should work
    let small_allocation = 1024 * 1024; // 1MB
    let can_allocate_small = monitor.can_allocate(small_allocation);
    
    println!("  üìä Attempting {} KB allocation", small_allocation / 1024);
    println!("  ‚úÖ Small allocation allowed: {}", can_allocate_small);
    
    assert!(can_allocate_small, "Small allocation should be allowed");
}

#[tokio::test]
async fn test_memory_pressure_handling() {
    println!("üß™ Testing memory pressure handling");
    
    let monitor = Arc::new(MemoryMonitor::new(10, 80)); // 10MB limit for testing
    
    // Fill up memory gradually
    let mut allocations = Vec::new();
    let chunk_size = 2 * 1024 * 1024; // 2MB chunks
    
    for i in 0..5 {
        match monitor.try_allocate(chunk_size) {
            Ok(allocation) => {
                println!("  ‚úÖ Allocated chunk {}: {} MB", i + 1, chunk_size / 1_048_576);
                println!("  üìä Memory usage: {:.1}%", monitor.usage_percent());
                allocations.push(allocation);
            }
            Err(e) => {
                println!("  üõ°Ô∏è  Allocation blocked at chunk {}: {}", i + 1, e);
                break;
            }
        }
    }
    
    // Check if critical threshold is detected
    if monitor.is_critical() {
        println!("  üö® Critical memory threshold detected");
    }
    
    // Clean up
    drop(allocations);
    println!("  üßπ Memory cleaned up: {} MB", monitor.current_usage_mb());
}

#[tokio::test]
async fn test_concurrent_memory_safety() {
    println!("üß™ Testing concurrent memory safety");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    let mut handles = Vec::new();
    
    // Spawn multiple tasks that allocate memory
    for i in 0..5 {
        let monitor_clone = monitor.clone();
        let handle = tokio::spawn(async move {
            let allocation_size = 1024 * 1024; // 1MB
            
            match monitor_clone.try_allocate(allocation_size) {
                Ok(_allocation) => {
                    println!("  ‚úÖ Task {} allocated {} KB", i, allocation_size / 1024);
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    // _allocation dropped here
                    println!("  üßπ Task {} cleaned up", i);
                }
                Err(e) => {
                    println!("  üõ°Ô∏è  Task {} allocation blocked: {}", i, e);
                }
            }
        });
        handles.push(handle);
    }
    
    // Wait for all tasks
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("  üìä Final memory usage: {} MB", monitor.current_usage_mb());
    
    // Memory should be cleaned up
    assert!(monitor.current_usage_mb() < 5, "Memory not properly cleaned up");
}

#[tokio::test]
async fn test_performance_benchmarks() {
    println!("üß™ Running performance benchmarks");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    
    // Benchmark memory allocation speed
    let start = Instant::now();
    let allocation_size = 64 * 1024; // 64KB
    
    let mut allocations = Vec::new();
    for i in 0..1000 {
        if let Ok(alloc) = monitor.try_allocate(allocation_size) {
            allocations.push(alloc);
        }
        
        if i % 100 == 0 {
            tokio::task::yield_now().await;
        }
    }
    
    let duration = start.elapsed();
    println!("  ‚ö° Allocated {} chunks in {:?}", allocations.len(), duration);
    println!("  üìä Rate: {:.1} allocations/ms", allocations.len() as f64 / duration.as_millis() as f64);
    
    // Clean up
    drop(allocations);
    
    // PERFORMANCE TARGET: Should handle 1000+ allocations quickly
    assert!(duration.as_millis() < 1000, "Allocation too slow: {:?}", duration);
}

#[tokio::test]
async fn test_embedder_integration() {
    println!("üß™ Testing embedder integration");
    
    // Test embedder creation (should fail gracefully without crashing)
    let result = StreamingNomicEmbedder::new_with_streaming("nonexistent_model.gguf").await;
    
    match result {
        Ok(embedder) => {
            println!("  ‚úÖ Embedder created successfully");
            
            // Test embedding (should not crash)
            let embedding_result = embedder.embed("test text");
            match embedding_result {
                Ok(embedding) => {
                    println!("  ‚úÖ Embedding created: {} dimensions", embedding.len());
                }
                Err(e) => {
                    println!("  ‚ÑπÔ∏è  Embedding failed (expected): {}", e);
                }
            }
        }
        Err(e) => {
            println!("  ‚ÑπÔ∏è  Embedder creation failed (expected without model): {}", e);
        }
    }
}

#[tokio::test]
async fn test_memory_leak_detection() {
    println!("üß™ Testing memory leak detection");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    let initial_usage = monitor.current_usage_mb();
    
    // Perform operations that might leak memory
    for i in 0..100 {
        let allocation = monitor.try_allocate(1024).unwrap(); // 1KB
        
        // Simulate work
        tokio::time::sleep(Duration::from_millis(1)).await;
        
        // Allocation should be dropped automatically
        drop(allocation);
        
        if i % 20 == 0 {
            let current_usage = monitor.current_usage_mb();
            println!("  üìä Iteration {}: {} MB", i, current_usage);
        }
    }
    
    let final_usage = monitor.current_usage_mb();
    let memory_diff = final_usage - initial_usage;
    
    println!("  üìä Memory difference: {} MB", memory_diff);
    
    // CRITICAL: Should not leak memory
    assert!(memory_diff < 1, "Memory leak detected: {} MB", memory_diff);
}

/// Helper function to simulate heavy memory operations
async fn simulate_heavy_operation(monitor: &Arc<MemoryMonitor>) -> Result<(), anyhow::Error> {
    // This simulates what the old implementation would do (fail)
    let large_size = 50 * 1024 * 1024; // 50MB
    
    if !monitor.can_allocate(large_size) {
        return Err(anyhow::anyhow!("Cannot allocate {} MB", large_size / 1_048_576));
    }
    
    let _allocation = monitor.try_allocate(large_size)?;
    Ok(())
}

#[tokio::test]
async fn test_real_world_scenario() {
    println!("üß™ Testing real-world scenario");
    
    let monitor = Arc::new(MemoryMonitor::for_nodejs());
    
    // Simulate loading a large model
    println!("  üìä Simulating large model loading...");
    
    // Old approach would fail here
    let heavy_operation_result = simulate_heavy_operation(&monitor).await;
    match heavy_operation_result {
        Ok(_) => println!("  ‚ö†Ô∏è  Heavy operation succeeded (unexpected)"),
        Err(e) => println!("  ‚úÖ Heavy operation blocked: {}", e),
    }
    
    // Streaming approach should work
    let streaming_size = StreamingGGUFLoader::MAX_WORKING_MEMORY;
    let streaming_result = monitor.try_allocate(streaming_size);
    
    match streaming_result {
        Ok(_allocation) => {
            println!("  ‚úÖ Streaming approach succeeded with {} KB", streaming_size / 1024);
        }
        Err(e) => {
            println!("  ‚ùå Streaming approach failed: {}", e);
        }
    }
}

/// Integration test runner
#[tokio::main]
async fn main() {
    println!("üöÄ Starting streaming memory benchmark suite");
    println!("================================================");
    
    // Run all tests
    test_streaming_memory_constraints().await;
    test_memory_allocation_limits().await;
    test_system_memory_monitoring().await;
    test_streaming_vs_traditional_memory_usage().await;
    test_v8_crash_prevention().await;
    test_memory_pressure_handling().await;
    test_concurrent_memory_safety().await;
    test_performance_benchmarks().await;
    test_embedder_integration().await;
    test_memory_leak_detection().await;
    test_real_world_scenario().await;
    
    println!("================================================");
    println!("üéâ All streaming memory tests completed!");
}

#[cfg(test)]
mod unit_tests {
    use super::*;
    
    #[test]
    fn test_memory_monitor_basics() {
        let monitor = MemoryMonitor::new(100, 80);
        
        assert_eq!(monitor.limit_mb(), 100);
        assert_eq!(monitor.current_usage_mb(), 0);
        assert!(!monitor.is_critical());
        
        // Test allocation
        let alloc = monitor.try_allocate(1024).unwrap();
        assert!(monitor.current_usage_mb() > 0);
        
        // Test cleanup
        drop(alloc);
        assert_eq!(monitor.current_usage_mb(), 0);
    }
    
    #[test]
    fn test_streaming_constants() {
        // Verify that streaming constants meet V8 safety requirements
        assert!(StreamingGGUFLoader::CHUNK_SIZE <= 65536);
        assert!(StreamingGGUFLoader::MAX_WORKING_MEMORY <= 1048576);
        assert!(StreamingGGUFLoader::DECODE_SIZE * 4 <= 65536); // 4 bytes per f32
    }
}