// END-TO-END PIPELINE VALIDATION - INTJ + Type 8 Ruthless Truth Testing
//
// MISSION: Validate complete RAG pipeline from file input to search results
// NO FALLBACKS: Every component must work or fail clearly
// PRINCIPLE 0: TRUTH ABOVE ALL - no simulations, only real functionality
//
// This test validates:
// 1. File ingestion â†’ Chunking â†’ Embedding â†’ Storage â†’ Search â†’ Results
// 2. Multi-file corpus processing with real files from the codebase
// 3. Search relevance validation with measurable metrics
// 4. Performance characteristics under realistic loads
// 5. Error propagation and recovery testing

use anyhow::{Result, Context};
use embed_search::{
    HybridSearch, semantic_chunker::SemanticChunker,
    GGUFEmbedder, GGUFEmbedderConfig, EmbeddingTask,
    simple_storage::VectorStorage, BM25Engine
};
use std::{
    fs, path::PathBuf, time::Instant, collections::HashMap
};
use tempfile::{tempdir, TempDir};
use walkdir::WalkDir;

/// VALIDATION CRITERIA FOR "WORKING" SYSTEM
/// 
/// FUNCTIONALITY (50 points):
/// - File ingestion works for markdown, Rust, Python, JS files
/// - Chunking preserves semantic boundaries and metadata
/// - Dual embedders select correctly based on file type
/// - Vector storage and retrieval maintains accuracy
/// - BM25 text search produces relevant keyword matches
/// - Hybrid fusion combines results effectively
/// - Symbol extraction identifies code structures
///
/// PERFORMANCE (30 points):
/// - Search latency < 100ms for typical queries
/// - Indexing speed > 10 files/second
/// - Memory usage stays reasonable (< 500MB for 100 files)
/// - Concurrent access doesn't degrade performance
///
/// RELIABILITY (20 points):
/// - Error conditions handled gracefully
/// - Missing models fail clearly with actionable messages
/// - Corrupted data doesn't crash the system
/// - Recovery from partial failures works

#[derive(Debug)]
struct PipelineValidationResults {
    functionality_score: f32,
    performance_score: f32, 
    reliability_score: f32,
    total_score: f32,
    detailed_results: HashMap<String, TestResult>,
}

#[derive(Debug, Clone)]
struct TestResult {
    passed: bool,
    duration_ms: u64,
    error_message: Option<String>,
    metrics: HashMap<String, f64>,
}

impl TestResult {
    fn success(duration_ms: u64) -> Self {
        Self {
            passed: true,
            duration_ms,
            error_message: None,
            metrics: HashMap::new(),
        }
    }

    fn failure(duration_ms: u64, error: String) -> Self {
        Self {
            passed: false,
            duration_ms,
            error_message: Some(error),
            metrics: HashMap::new(),
        }
    }

    fn with_metric(mut self, key: &str, value: f64) -> Self {
        self.metrics.insert(key.to_string(), value);
        self
    }
}

/// BRUTAL TRUTH VALIDATOR: Complete End-to-End Pipeline Test
#[tokio::test]
async fn test_complete_rag_pipeline_validation() -> Result<()> {
    println!("ðŸ”¥ COMPLETE RAG PIPELINE VALIDATION - INTJ + Type 8 BRUTAL TRUTH MODE");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    let start_time = Instant::now();
    let mut results = PipelineValidationResults {
        functionality_score: 0.0,
        performance_score: 0.0,
        reliability_score: 0.0,
        total_score: 0.0,
        detailed_results: HashMap::new(),
    };

    // Phase 1: Test File Ingestion and Processing
    println!("\nðŸ“ PHASE 1: FILE INGESTION AND PROCESSING");
    let ingestion_result = test_file_ingestion_pipeline().await?;
    results.detailed_results.insert("file_ingestion".to_string(), ingestion_result.clone());
    if ingestion_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… File ingestion: PASSED");
    } else {
        println!("âŒ File ingestion: FAILED - {}", 
                ingestion_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 2: Test Chunking and Metadata Extraction
    println!("\nðŸ”¨ PHASE 2: CHUNKING AND METADATA EXTRACTION");
    let chunking_result = test_chunking_pipeline().await?;
    results.detailed_results.insert("chunking".to_string(), chunking_result.clone());
    if chunking_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Chunking: PASSED");
    } else {
        println!("âŒ Chunking: FAILED - {}", 
                chunking_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 3: Test Dual Embedder Selection
    println!("\nðŸ§  PHASE 3: DUAL EMBEDDER SELECTION");
    let embedding_result = test_dual_embedder_pipeline().await?;
    results.detailed_results.insert("embedding".to_string(), embedding_result.clone());
    if embedding_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Dual embedder selection: PASSED");
    } else {
        println!("âŒ Dual embedder selection: FAILED - {}", 
                embedding_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 4: Test Vector Storage and Retrieval
    println!("\nðŸ’¾ PHASE 4: VECTOR STORAGE AND RETRIEVAL");
    let storage_result = test_vector_storage_pipeline().await?;
    results.detailed_results.insert("storage".to_string(), storage_result.clone());
    if storage_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Vector storage: PASSED");
    } else {
        println!("âŒ Vector storage: FAILED - {}", 
                storage_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 5: Test BM25 Text Search
    println!("\nðŸ” PHASE 5: BM25 TEXT SEARCH");
    let bm25_result = test_bm25_search_pipeline().await?;
    results.detailed_results.insert("bm25".to_string(), bm25_result.clone());
    if bm25_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… BM25 text search: PASSED");
    } else {
        println!("âŒ BM25 text search: FAILED - {}", 
                bm25_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 6: Test Hybrid Search Fusion
    println!("\nâš¡ PHASE 6: HYBRID SEARCH FUSION");
    let fusion_result = test_hybrid_search_pipeline().await?;
    results.detailed_results.insert("fusion".to_string(), fusion_result.clone());
    if fusion_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Hybrid search fusion: PASSED");
    } else {
        println!("âŒ Hybrid search fusion: FAILED - {}", 
                fusion_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 7: Test Multi-File Corpus Processing
    println!("\nðŸ“š PHASE 7: MULTI-FILE CORPUS PROCESSING");
    let corpus_result = test_corpus_processing_pipeline().await?;
    results.detailed_results.insert("corpus".to_string(), corpus_result.clone());
    if corpus_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Multi-file corpus: PASSED");
    } else {
        println!("âŒ Multi-file corpus: FAILED - {}", 
                corpus_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 8: Test Search Relevance
    println!("\nðŸŽ¯ PHASE 8: SEARCH RELEVANCE VALIDATION");
    let relevance_result = test_search_relevance_pipeline().await?;
    results.detailed_results.insert("relevance".to_string(), relevance_result.clone());
    if relevance_result.passed {
        results.functionality_score += 10.0;
        println!("âœ… Search relevance: PASSED");
    } else {
        println!("âŒ Search relevance: FAILED - {}", 
                relevance_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 9: Test Performance Under Load
    println!("\nâš¡ PHASE 9: PERFORMANCE UNDER LOAD");
    let performance_result = test_performance_pipeline().await?;
    results.detailed_results.insert("performance".to_string(), performance_result.clone());
    if performance_result.passed {
        results.performance_score += 30.0;
        println!("âœ… Performance under load: PASSED");
    } else {
        println!("âŒ Performance under load: FAILED - {}", 
                performance_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Phase 10: Test Error Conditions and Recovery
    println!("\nðŸ›¡ï¸ PHASE 10: ERROR CONDITIONS AND RECOVERY");
    let error_result = test_error_recovery_pipeline().await?;
    results.detailed_results.insert("error_recovery".to_string(), error_result.clone());
    if error_result.passed {
        results.reliability_score += 20.0;
        println!("âœ… Error recovery: PASSED");
    } else {
        println!("âŒ Error recovery: FAILED - {}", 
                error_result.error_message.as_deref().unwrap_or("Unknown error"));
    }

    // Calculate final scores
    results.total_score = results.functionality_score + results.performance_score + results.reliability_score;
    let total_duration = start_time.elapsed();

    // BRUTAL TRUTH REPORTING
    println!("\nðŸ”¥ FINAL VALIDATION RESULTS - BRUTAL TRUTH ASSESSMENT");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ðŸ“Š FUNCTIONALITY SCORE: {:.1}/80.0 ({:.1}%)", 
             results.functionality_score, (results.functionality_score / 80.0) * 100.0);
    println!("âš¡ PERFORMANCE SCORE:   {:.1}/30.0 ({:.1}%)", 
             results.performance_score, (results.performance_score / 30.0) * 100.0);
    println!("ðŸ›¡ï¸ RELIABILITY SCORE:   {:.1}/20.0 ({:.1}%)", 
             results.reliability_score, (results.reliability_score / 20.0) * 100.0);
    println!("ðŸŽ¯ TOTAL SCORE:         {:.1}/130.0 ({:.1}%)", 
             results.total_score, (results.total_score / 130.0) * 100.0);
    println!("â±ï¸ TOTAL DURATION:       {:?}", total_duration);

    // Detailed breakdown
    println!("\nðŸ“‹ DETAILED TEST BREAKDOWN:");
    for (test_name, result) in &results.detailed_results {
        let status = if result.passed { "âœ… PASS" } else { "âŒ FAIL" };
        println!("   {} {}: {}ms", status, test_name, result.duration_ms);
        
        if !result.passed {
            if let Some(error) = &result.error_message {
                println!("      Error: {}", error);
            }
        }
        
        for (metric, value) in &result.metrics {
            println!("      {}: {:.3}", metric, value);
        }
    }

    // TRUTH ASSESSMENT
    println!("\nðŸ”¥ TRUTH ASSESSMENT:");
    if results.total_score >= 117.0 { // 90%
        println!("ðŸš€ VERDICT: PRODUCTION READY - All systems operational");
    } else if results.total_score >= 104.0 { // 80%
        println!("âš ï¸  VERDICT: NEEDS IMPROVEMENT - Critical issues remain");
    } else if results.total_score >= 91.0 { // 70%
        println!("ðŸš§ VERDICT: DEVELOPMENT STAGE - Major functionality missing");
    } else {
        println!("ðŸ’¥ VERDICT: BROKEN - Fundamental system failures");
    }

    // Return success only if we pass the minimum threshold
    if results.total_score >= 91.0 {
        Ok(())
    } else {
        Err(anyhow::anyhow!(
            "Pipeline validation FAILED: Score {:.1}/130.0 below minimum threshold (91.0/130.0)",
            results.total_score
        ))
    }
}

// Individual test implementations
async fn test_file_ingestion_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Create test files with different types
    let temp_dir = tempdir().context("Failed to create temp directory")?;
    let test_files = create_test_file_corpus(&temp_dir)?;
    
    // Test file reading and type detection
    let mut processed_files = 0;
    let mut detected_types = HashMap::new();
    
    for file_path in &test_files {
        if let Ok(content) = fs::read_to_string(file_path) {
            processed_files += 1;
            
            // Detect file type based on extension
            if let Some(ext) = file_path.extension().and_then(|e| e.to_str()) {
                *detected_types.entry(ext.to_string()).or_insert(0) += 1;
            }
            
            // Basic validation - file should have content
            if content.trim().is_empty() {
                return Ok(TestResult::failure(
                    start.elapsed().as_millis() as u64,
                    format!("Empty file detected: {:?}", file_path)
                ));
            }
        }
    }
    
    if processed_files != test_files.len() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Could not read all files: {}/{}", processed_files, test_files.len())
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("files_processed", processed_files as f64)
        .with_metric("file_types", detected_types.len() as f64))
}

async fn test_chunking_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Test semantic chunking with real code
    let rust_code = r#"
use std::collections::HashMap;

/// User management system
pub struct UserManager {
    users: HashMap<String, User>,
    active_sessions: Vec<String>,
}

impl UserManager {
    pub fn new() -> Self {
        Self {
            users: HashMap::new(),
            active_sessions: Vec::new(),
        }
    }
    
    pub fn add_user(&mut self, user: User) -> Result<(), UserError> {
        if self.users.contains_key(&user.email) {
            return Err(UserError::DuplicateEmail);
        }
        self.users.insert(user.email.clone(), user);
        Ok(())
    }
}

#[derive(Debug)]
pub struct User {
    pub email: String,
    pub name: String,
    pub created_at: u64,
}
"#;

    let mut chunker = SemanticChunker::new(1000)
        .map_err(|e| anyhow::anyhow!("Failed to create chunker: {}", e))?;
    
    let chunks = chunker.chunk_code(rust_code, "test.rs", "rs")
        .map_err(|e| anyhow::anyhow!("Chunking failed: {}", e))?;
    
    if chunks.is_empty() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "No chunks produced from valid code".to_string()
        ));
    }
    
    // Validate chunk quality
    let has_struct_chunk = chunks.iter().any(|c| c.symbols.contains(&"UserManager".to_string()));
    let has_method_chunk = chunks.iter().any(|c| c.symbols.contains(&"add_user".to_string()));
    
    if !has_struct_chunk || !has_method_chunk {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "Chunks missing expected symbols".to_string()
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("chunks_produced", chunks.len() as f64)
        .with_metric("symbols_extracted", 
                     chunks.iter().map(|c| c.symbols.len()).sum::<usize>() as f64))
}

async fn test_dual_embedder_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Test that we can create both text and code embedders
    let text_config = GGUFEmbedderConfig {
        model_path: "./src/model/nomic-embed-text-v1.5.Q4_K_M.gguf".to_string(),
        cache_size: 10,
        batch_size: 2,
        ..Default::default()
    };
    
    let code_config = GGUFEmbedderConfig {
        model_path: "./src/model/nomic-embed-code.Q4_K_M.gguf".to_string(), 
        cache_size: 10,
        batch_size: 2,
        ..Default::default()
    };
    
    let text_embedder = match GGUFEmbedder::new(text_config) {
        Ok(embedder) => embedder,
        Err(e) => return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Text embedder creation failed: {}", e)
        ))
    };
    
    let code_embedder = match GGUFEmbedder::new(code_config) {
        Ok(embedder) => embedder,
        Err(e) => return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Code embedder creation failed: {}", e)
        ))
    };
    
    // Test embeddings are different for same content
    let test_content = "function calculate(x) { return x * 2; }";
    
    let text_embedding = text_embedder.embed(test_content, EmbeddingTask::SearchDocument)
        .map_err(|e| anyhow::anyhow!("Text embedding failed: {}", e))?;
    let code_embedding = code_embedder.embed(test_content, EmbeddingTask::CodeDefinition)
        .map_err(|e| anyhow::anyhow!("Code embedding failed: {}", e))?;
    
    if text_embedding.len() != 768 || code_embedding.len() != 768 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "Incorrect embedding dimensions".to_string()
        ));
    }
    
    // Calculate similarity
    let similarity = cosine_similarity(&text_embedding, &code_embedding);
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("text_embedding_dim", text_embedding.len() as f64)
        .with_metric("code_embedding_dim", code_embedding.len() as f64)
        .with_metric("cross_embedder_similarity", similarity as f64))
}

async fn test_vector_storage_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    let temp_dir = tempdir()?;
    let db_path = temp_dir.path().join("test_vector.db");
    
    let mut storage = VectorStorage::new(db_path.to_str().unwrap())
        .map_err(|e| anyhow::anyhow!("Storage creation failed: {}", e))?;
    
    // Create test embeddings
    let embeddings = vec![
        (vec![0.1, 0.2, 0.3], "doc1", "Test document 1"),
        (vec![0.4, 0.5, 0.6], "doc2", "Test document 2"),
        (vec![0.7, 0.8, 0.9], "doc3", "Test document 3"),
    ];
    
    // Store embeddings (adjust to match VectorStorage API)
    for (embedding, doc_id, content) in &embeddings {
        let contents = vec![content.to_string()];
        let embeddings_vec = vec![embedding.clone()];
        let file_paths = vec![format!("{}.txt", doc_id)];
        storage.store(contents, embeddings_vec, file_paths)
            .map_err(|e| anyhow::anyhow!("Storage failed: {}", e))?;
    }
    
    // Test retrieval
    let query_embedding = vec![0.15, 0.25, 0.35];
    let results = storage.search(query_embedding, 2)
        .map_err(|e| anyhow::anyhow!("Search failed: {}", e))?;
    
    if results.is_empty() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "No search results returned".to_string()
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("embeddings_stored", embeddings.len() as f64)
        .with_metric("results_returned", results.len() as f64))
}

async fn test_bm25_search_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    let mut bm25 = BM25Engine::new()
        .map_err(|e| anyhow::anyhow!("BM25 creation failed: {}", e))?;
    
    // Add test documents
    let documents = vec![
        ("doc1", "Rust programming language systems development"),
        ("doc2", "Python machine learning data science"),
        ("doc3", "JavaScript web development frontend backend"),
        ("doc4", "Go concurrent programming microservices"),
    ];
    
    for (doc_id, content) in &documents {
        bm25.index_document(doc_id, content);
    }
    
    // Test search
    let results = bm25.search("programming", 3)
        .map_err(|e| anyhow::anyhow!("BM25 search failed: {}", e))?;
    
    if results.is_empty() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "No BM25 results returned".to_string()
        ));
    }
    
    // Verify relevance - "programming" should match multiple docs
    let programming_matches = results.iter()
        .filter(|r| r.snippet.contains("programming"))
        .count();
    
    if programming_matches == 0 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "BM25 results not relevant to query".to_string()
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("documents_indexed", documents.len() as f64)
        .with_metric("results_returned", results.len() as f64)
        .with_metric("relevant_results", programming_matches as f64))
}

async fn test_hybrid_search_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    let temp_dir = tempdir()?;
    let db_path = temp_dir.path().join("hybrid_test.db");
    
    let mut hybrid_search = HybridSearch::new(db_path.to_str().unwrap()).await
        .map_err(|e| anyhow::anyhow!("Hybrid search creation failed: {}", e))?;
    
    // Index test content
    let contents = vec![
        "Rust systems programming memory safety".to_string(),
        "Python data analysis machine learning".to_string(),
        "JavaScript async programming promises".to_string(),
    ];
    let paths = vec![
        "test.rs".to_string(),
        "analysis.py".to_string(), 
        "app.js".to_string(),
    ];
    
    hybrid_search.index(contents.clone(), paths.clone()).await
        .map_err(|e| anyhow::anyhow!("Indexing failed: {}", e))?;
    
    // Test hybrid search
    let results = hybrid_search.search("programming", 5).await
        .map_err(|e| anyhow::anyhow!("Hybrid search failed: {}", e))?;
    
    if results.is_empty() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "No hybrid search results".to_string()
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("files_indexed", contents.len() as f64)
        .with_metric("search_results", results.len() as f64))
}

async fn test_corpus_processing_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Use actual codebase files for testing
    let mut file_count = 0;
    let mut total_size = 0u64;
    
    for entry in WalkDir::new("src").max_depth(3) {
        if let Ok(entry) = entry {
            if entry.file_type().is_file() {
                if let Some(ext) = entry.path().extension().and_then(|e| e.to_str()) {
                    if matches!(ext, "rs" | "py" | "js" | "md") {
                        if let Ok(metadata) = entry.metadata() {
                            file_count += 1;
                            total_size += metadata.len();
                            
                            // Stop at reasonable limit for test
                            if file_count >= 20 {
                                break;
                            }
                        }
                    }
                }
            }
        }
    }
    
    if file_count < 5 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "Insufficient test files found in codebase".to_string()
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("files_discovered", file_count as f64)
        .with_metric("total_size_bytes", total_size as f64))
}

async fn test_search_relevance_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Test search relevance with known content
    let temp_dir = tempdir()?;
    let db_path = temp_dir.path().join("relevance_test.db");
    
    let mut search = HybridSearch::new(db_path.to_str().unwrap()).await
        .map_err(|e| anyhow::anyhow!("Search creation failed: {}", e))?;
    
    // Create documents with known relationships
    let contents = vec![
        "fn fibonacci(n: u32) -> u32 { if n <= 1 { n } else { fibonacci(n-1) + fibonacci(n-2) } }".to_string(),
        "def calculate_fibonacci(n): return n if n <= 1 else calculate_fibonacci(n-1) + calculate_fibonacci(n-2)".to_string(),
        "function fibonacci(n) { return n <= 1 ? n : fibonacci(n-1) + fibonacci(n-2); }".to_string(),
        "struct User { name: String, age: u32 }".to_string(),
        "class Database { constructor() { this.connection = null; } }".to_string(),
    ];
    
    let paths = vec!["fib.rs".to_string(), "fib.py".to_string(), "fib.js".to_string(), "user.rs".to_string(), "db.js".to_string()];
    
    search.index(contents.clone(), paths.clone()).await
        .map_err(|e| anyhow::anyhow!("Relevance test indexing failed: {}", e))?;
    
    // Test queries with expected relevance
    let test_queries = vec![
        ("fibonacci", 3), // Should match first 3 files
        ("User struct", 1), // Should match user.rs
        ("database", 1), // Should match db.js
    ];
    
    let mut relevance_score = 0.0;
    let mut total_queries = 0;
    
    for (query, expected_relevant) in test_queries {
        let results = search.search(query, 5).await
            .map_err(|e| anyhow::anyhow!("Relevance search failed: {}", e))?;
        
        total_queries += 1;
        
        // Count actually relevant results
        let relevant_count = match query {
            "fibonacci" => results.iter().filter(|r| r.content.contains("fibonacci")).count(),
            "User struct" => results.iter().filter(|r| r.content.contains("User")).count(),
            "database" => results.iter().filter(|r| r.content.to_lowercase().contains("database")).count(),
            _ => 0,
        };
        
        if relevant_count >= expected_relevant {
            relevance_score += 1.0;
        }
    }
    
    let final_relevance = relevance_score / total_queries as f64;
    
    if final_relevance < 0.8 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Poor search relevance: {:.2}", final_relevance)
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("relevance_score", final_relevance)
        .with_metric("queries_tested", total_queries as f64))
}

async fn test_performance_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    let temp_dir = tempdir()?;
    let db_path = temp_dir.path().join("perf_test.db");
    
    let mut search = HybridSearch::new(db_path.to_str().unwrap()).await
        .map_err(|e| anyhow::anyhow!("Performance test search creation failed: {}", e))?;
    
    // Generate test data
    let mut contents = Vec::new();
    let mut paths = Vec::new();
    
    for i in 0..50 {
        let content = format!(
            "fn test_function_{}() {{ println!(\"Function number {}\"); let result = {}; result }}",
            i, i, i * 2
        );
        contents.push(content);
        paths.push(format!("test_{}.rs", i));
    }
    
    // Measure indexing performance
    let index_start = Instant::now();
    search.index(contents.clone(), paths.clone()).await
        .map_err(|e| anyhow::anyhow!("Performance indexing failed: {}", e))?;
    let index_duration = index_start.elapsed();
    
    let files_per_second = contents.len() as f64 / index_duration.as_secs_f64();
    
    if files_per_second < 5.0 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Indexing too slow: {:.2} files/sec", files_per_second)
        ));
    }
    
    // Measure search performance
    let search_start = Instant::now();
    let _results = search.search("test_function", 10).await
        .map_err(|e| anyhow::anyhow!("Performance search failed: {}", e))?;
    let search_duration = search_start.elapsed();
    
    if search_duration.as_millis() > 500 {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            format!("Search too slow: {}ms", search_duration.as_millis())
        ));
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("indexing_files_per_sec", files_per_second)
        .with_metric("search_latency_ms", search_duration.as_millis() as f64)
        .with_metric("files_indexed", contents.len() as f64))
}

async fn test_error_recovery_pipeline() -> Result<TestResult> {
    let start = Instant::now();
    
    // Test 1: Invalid model path
    let bad_config = GGUFEmbedderConfig {
        model_path: "./nonexistent/model.gguf".to_string(),
        ..Default::default()
    };
    
    let embedder_result = GGUFEmbedder::new(bad_config);
    if embedder_result.is_ok() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "Should have failed with nonexistent model path".to_string()
        ));
    }
    
    // Test 2: Invalid database path
    let invalid_db_result = HybridSearch::new("/invalid/path/db.sqlite").await;
    if invalid_db_result.is_ok() {
        return Ok(TestResult::failure(
            start.elapsed().as_millis() as u64,
            "Should have failed with invalid database path".to_string()
        ));
    }
    
    // Test 3: Empty search query handling
    let temp_dir = tempdir()?;
    let db_path = temp_dir.path().join("error_test.db");
    
    if let Ok(mut search) = HybridSearch::new(db_path.to_str().unwrap()).await {
        let empty_result = search.search("", 5).await;
        // This should either return empty results or handle gracefully
        match empty_result {
            Ok(_results) => {
                // Empty query should return empty results or all results, not crash
                // This is acceptable behavior
            },
            Err(_) => {
                // Also acceptable - empty query validation
            }
        }
    }
    
    Ok(TestResult::success(start.elapsed().as_millis() as u64)
        .with_metric("error_conditions_tested", 3.0))
}

// Utility functions
fn create_test_file_corpus(temp_dir: &TempDir) -> Result<Vec<PathBuf>> {
    let mut files = Vec::new();
    
    // Create Rust file
    let rust_file = temp_dir.path().join("test.rs");
    fs::write(&rust_file, r#"
fn main() {
    println!("Hello, world!");
}

struct TestStruct {
    field: i32,
}

impl TestStruct {
    fn new(value: i32) -> Self {
        Self { field: value }
    }
}
"#)?;
    files.push(rust_file);
    
    // Create Python file
    let python_file = temp_dir.path().join("test.py");
    fs::write(&python_file, r#"
def hello_world():
    print("Hello, world!")

class TestClass:
    def __init__(self, value):
        self.value = value
    
    def get_value(self):
        return self.value
"#)?;
    files.push(python_file);
    
    // Create JavaScript file
    let js_file = temp_dir.path().join("test.js");
    fs::write(&js_file, r#"
function helloWorld() {
    console.log("Hello, world!");
}

class TestClass {
    constructor(value) {
        this.value = value;
    }
    
    getValue() {
        return this.value;
    }
}
"#)?;
    files.push(js_file);
    
    // Create Markdown file
    let md_file = temp_dir.path().join("test.md");
    fs::write(&md_file, r#"
# Test Document

This is a test markdown document.

## Features

- Item 1
- Item 2
- Item 3

### Code Example

```rust
fn example() {
    println!("Example code");
}
```
"#)?;
    files.push(md_file);
    
    Ok(files)
}

fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return 0.0;
    }
    
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    
    if norm_a == 0.0 || norm_b == 0.0 {
        0.0
    } else {
        dot_product / (norm_a * norm_b)
    }
}