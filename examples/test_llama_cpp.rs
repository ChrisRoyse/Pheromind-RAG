// examples/test_llama_cpp.rs - Test basic llama-cpp-2 functionality

use anyhow::Result;
use std::path::Path;

fn main() -> Result<()> {
    println!("ü¶ô Testing llama-cpp-2 Integration");
    println!("==================================\n");
    
    println!("üì¶ Testing llama-cpp-2 library availability...");
    
    // Note: The actual llama-cpp-2 v0.1.54 API may differ from newer versions
    // This example demonstrates that the library is correctly linked
    
    // Import the crate to verify it's available
    use llama_cpp_2 as _llama;
    use llama_cpp_sys_2 as _llama_sys;
    
    println!("‚úÖ llama-cpp-2 library is available");
    println!("‚úÖ llama-cpp-sys-2 library is available\n");
    
    // Check for model file
    let model_path = "./src/model/nomic-embed-code.Q4_K_M.gguf";
    
    if !Path::new(model_path).exists() {
        println!("‚ö†Ô∏è  Model file not found at: {}", model_path);
        println!("   Please download the model first:");
        println!("   wget https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf");
        println!("   mkdir -p src/model");
        println!("   mv nomic-embed-text-v1.5.Q4_K_M.gguf src/model/nomic-embed-code.Q4_K_M.gguf\n");
    } else {
        println!("üìÇ Model file found: {}", model_path);
    }
    
    // Check GPU features
    println!("üîç GPU Feature Detection:");
    
    #[cfg(feature = "cuda")]
    println!("   ‚úÖ CUDA support enabled");
    #[cfg(not(feature = "cuda"))]
    println!("   ‚ùå CUDA support not enabled");
    
    #[cfg(feature = "metal")]
    println!("   ‚úÖ Metal support enabled");
    #[cfg(not(feature = "metal"))]
    println!("   ‚ùå Metal support not enabled");
    
    #[cfg(feature = "hipblas")]
    println!("   ‚úÖ HIPBlas/ROCm support enabled");
    #[cfg(not(feature = "hipblas"))]
    println!("   ‚ùå HIPBlas/ROCm support not enabled");
    
    println!("\n‚ú® Basic integration test complete!");
    println!("\nNote: For full llama-cpp-2 API usage, refer to the crate documentation.");
    println!("      The API varies between versions, so check: https://docs.rs/llama-cpp-2/0.1.54");
    
    Ok(())
}